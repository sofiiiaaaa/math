---
sidebar_position: 1
---

# Введение

import Figure from '@components/Figure'

import Formula from '@components/Formula'

import Highlight from '@components/Highlight'

import useBaseUrl from '@docusaurus/useBaseUrl';

## Размещения

Число размещений $A_n^k$, по определению, равно количеству способов, с помощью которых из $n$ предметов можно извлечь
$k (k < n)$. Порядок расположения предметов существенен, то есть предметы различимы. Например, это очереди и тому
подобное.

Справедлива формула

<Formula description="(1)">

$A_n^k=n(n-1)(n-2) \ldots (n-k+1)$

</Formula>

Если применить понятие факториала $n!=1 \cdot 2 \cdot 3 \cdot \ldots \cdot (n-1) \cdot n$, то можно записать

<Formula description="(2)">

$A_n^k= \cfrac{n!}{(n-k)!}$

</Formula>

## Перестановки

По определению

<Formula description="(3)">

$P_n=A_n^n=n!$

</Formula>

равно числу способов, с помощью которых можно построить различные последовательности из $n$ предметов.

## Сочетания

Это число $C_n^k$ появляется при решении задачи: сколькими способами можно извлечь $k$ предметов из $n$ возможных
$0 \le k \le n$, если порядок роли не играет, то есть предметы не различимы.

Воспользуемся тем, что нам уже известно. Предположим, что $C_n^k$ нам известно, а ищем мы число размещений $A_n^k$.
Поступим следующим образом. Найдем сначала все сочетания $C_n^k$ по $k$ предметам, затем в каждом сочетании устроим
перестановки $P_k$ всех $k$ предметов. В результате получим все размещения $A_n^k$. В формальной записи это выглядит
так: $C_n^k \cdot P_k = A_n^k$.

Отсюда

<Formula description="(4)">

$C_n^k = \cfrac{A_n^k}{P_k} = \cfrac{n!}{k!(n-k)!}$

</Formula>

Для больших значений $n$ и $k$ приведенные формулы становятся весьма громоздкими и поэтому для практики по существу
бесполезны. В этом случае приходит на помощь т.н. формулы Стирлинга

<Formula description="(5)">

$n! \approx n^n \sqrt{2\pi n} \cdot e^{-n}$

</Formula>

Это равенство верно асимптотически, то есть относительная погрешность тем меньше, чем больше $n$.

Формула **(5)** дает весьма малые относительные погрешности даже при небольших $n$. Так, для $1!$, $2!$, $5!$ получаем
приближенные значения $0.9221$, $1.919$, $118.019$, что соответствует относительным ошибкам $8\%$, $4\%$, $2\%$
соответственно. Для больших $n$, по формуле **(5)**, например $100!$ относительная ошибка уже равна $0.08\%$.

Более точной является формула

<Formula description="(6)">

$n! \approx \sqrt{2\pi} \cdot n^{(n+\frac{1}{2})} \cdot exp(-n + \cfrac{\theta}{12n}), 0 < \theta < 1$

</Formula>

При $\theta = 1$ получим

<Formula description="(7)">

$n! \approx \sqrt{2\pi} \cdot n^{(n+\frac{1}{2})} \cdot exp(-n + \cfrac{1}{12n}), 0 < \theta < 1$

</Formula>

Формула **(5)** дает приближение с недостатком, формула **(7)** с избытком, но ее относительная погрешность меньше. При
$n=1 \space 1! \approx 1.0023$ - ошибка в $0.2\%$ вместо $8\%$, то есть в 40 раз меньше, при $n=10$ уже первые пять цифр
верные и погрешность меньше в $400$ раз.

Для более свободного понимания теории вероятностей нужно знать символику, связанную с множествами и операциями над ними.

Множества (события) обозначаются буквами $A$, $B$, $C$ и т.д.

- $\Omega$ &mdash; достоверное событие.
- $\varnothing$ &mdash; невозможное событие.
- $A \subset B$ &mdash; $A$ включено в $B$, событие $A$ влечет за собой событие $B$.
- $A=B$ &mdash; события $A$ и $B$ эквивалентны.
- $A \cdot B, A \cap B$ &mdash; умножение событий, одновременное наблюдение обоих событий.
- $A \cup B$ &mdash; происходит либо одно событие, либо другое, либо вместе;
- $A+B$ &mdash; ставится, если это прямая сумма, то есть $A \cup B = 0$
- $A-B, A \setminus B$ &mdash; разность событий.
- $\bar{A}$ &mdash; отрицание

Два события $A$, $\bar{A}$ взаимно противоположны тогда и только тогда, когда

<Formula description="">

$
\begin{cases}
   A+\bar{A}=\Omega \\
   A \cdot \bar{A} = \varnothing
\end{cases}
$

</Formula>

## Классическое определение вероятности

Понятие "вероятность" возникло давно и претерпевало всевозможные изменения в своем понимании, даже живущие в одно время
исследователи могли понимать его по-разному. Но всегда было общее: это представление о вероятности, как об отношении
части к целому.

Как набор эмпирических наблюдений теория вероятностей, можно сказать, существует столько, сколько существуют азартные
игры. Математики, которые занимались подсчетом шансов игроков, прекрасно понимали, что речь идет не только об игре.

Определение вероятности при игре в кости сводилось к нахождению числа различных исходов. Их доля по отношению к общему
числу исходов в современном понимании и есть вероятность.

Пусть имеется $n$ возможных исходов $E_1,E_2, \ldots, E_n$,события считаются попарно несовместными. Это означает, что
если наступил какой-либо исход, например, $E_1, то ни один из оставшихся исходов в этом испытании наступить уже не
может. Каждый из таких исходов будет называться **элементарным событием**.

Наряду с ними будем рассматривать **случайные события**. Допустим $A$ реализуется, если происходит какое-либо число $m$
элементарных исходов $m \le n$, тогда вероятностью события $A$ назовем отношение

<Formula description="">

$P(A)=\cfrac{m}{n}$

</Formula>

где $n$ &mdash; общее число исходов, $m$ &mdash; число благоприятствующих исходов.

Это формула классического определения вероятности.
